{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "CATALOG_URL = \"http://server:8181/catalog\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = \".\".join(SPARK_VERSION.split(\".\")[:2])\n",
    "ICEBERG_VERSION = \"1.8.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    \"http://server:8181/management/v1/warehouse\",\n",
    "    json={\n",
    "        \"warehouse-name\": \"hdfs\",\n",
    "        \"storage-profile\": {\n",
    "            \"type\": \"hdfs\",\n",
    "            \"key-prefix\": \"/user/hdfs\",\n",
    "            \"url\": \"hdfs://namenode:8020\",\n",
    "        },\n",
    "    },\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    \"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    \"spark.sql.catalog.lakekeeper.warehouse\": \"hdfs\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_config = SparkConf().setMaster(\"local\").setAppName(\"Iceberg-REST\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "spark.sql(\"USE lakekeeper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Write Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS my_namespace\")\n",
    "spark.sql(\"SHOW NAMESPACES\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([[1, \"a-string\", 2.2]], columns=[\"id\", \"strings\", \"floats\"])\n",
    "sdf = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.writeTo(f\"my_namespace.my_table\").createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM my_namespace.my_table\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
